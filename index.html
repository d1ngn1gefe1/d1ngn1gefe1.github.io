<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Zelun (Alan) Luo</title>

  <!-- Latest compiled and minified CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
        integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  <!-- Custom CSS -->
  <link href="sass/style.css" rel="stylesheet">
</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap
scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

<!-- Navigation -->
<nav class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header page-scroll">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand page-scroll" href="#page-top">Alan Zelun Luo</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse navbar-ex1-collapse">
      <ul class="nav navbar-nav">
        <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
        <li class="hidden">
          <a class="page-scroll" href="#page-top"></a>
        </li>
        <li>
          <a class="page-scroll" href="#about">About</a>
        </li>
        <li>
          <a class="page-scroll" href="#services">Publications</a>
        </li>
        <li>
          <a class="page-scroll" href="#contact">Projects</a>
        </li>
      </ul>
    </div>
    <!-- /.navbar-collapse -->
  </div>
  <!-- /.container -->
</nav>

<!-- Intro Section -->
<section id="intro" class="intro-section">
  <div class="container">
    <div class="row">
      <div class="col-sm-4 col-sm-offset-4 my-info">
        <h1>Zelun (Alan) Luo</h1>
        <p><br>Stanford Vision Lab<br>Advisor: Prof Fei-Fei Li<br>Stanford University<br><br>zelunluo at stanford dot edu</p>
        <p>
          <a href="https://scholar.google.com/citations?user=y90O9pUAAAAJ&hl=en">[Google Scholar]</a>
          <a href="https://github.com/d1ngn1gefe1">[Github]</a>
          <a href="intro/cv.pdf">[CV (updated on Dec 2016)]</a>
        </p>
      </div>
    </div>
  </div>
</section>

<!-- About Section -->
<section id="about" class="about-section">
  <div class="container">
    <div class="row">
      <h1 class="subtitle">About</h1>
    </div>

    <div class="row">
      <div class="col-sm-8">
        <h3 class="text-left">Education</h3>
        <div class="col-sm-6">
          <img src="about/stanford_seal.png" class="school-fig">
          <p>Master of Science<br>
            Computer Science<br>
            Stanford University<br>
            2015 - Present
          </p>
        </div>
        <div class="col-sm-6">
          <img src="about/uiuc_seal.png" class="school-fig">
          <p>Bachelor of Science<br>
            Electrical and Computer Engineering<br>
            University of Illinois at Urbana-Champaign<br>
            2012 - 2015
          </p>
        </div>
      </div>
    </div>

    <div class="row">
      <div class="col-sm-8">
        <h3 class="text-left">Industry</h3>
        <div class="col-sm-4">
          <img src="about/google.png" class="company-fig img-circle">
          <p>Google<br>
            Research Intern<br>
            June 2017 - Present
          </p>
        </div>
        <div class="col-sm-4">
          <img src="about/a9.png" class="company-fig img-circle">
          <p>Amazon A9<br>
            Research Intern<br>
            June 2016 - September 2016
          </p>
        </div>
        <div class="col-sm-4">
          <img src="about/yahoo.png" class="company-fig img-circle">
          <p>Yahoo<br>
            Software Engineering Intern<br>
            May 2015 - September 2015
          </p>
        </div>
      </div>
    </div>

    <div class="row">
      <h3 class="text-left">Teaching</h3>
      <ul>
        <li><p class="text-left">Teaching Assistant, CS 231N (Convolutional Neural Networks for Visual Recognition), Spring 2017</p></li>
        <li><p class="text-left">Teaching Assistant, CS 224N (Natural Language Processing with Deep Learning), Winter 2017</p></li>
        <li><p class="text-left">Head Teaching Assistant, CS 131 (Computer Vision: Foundations and Applications), Fall 2016</p></li>
        <li><p class="text-left">Teaching Assistant, CS 109 (Probability for Computer Scientists), Spring 2016</p></li>
        <li><p class="text-left">Teaching Assistant, CS 109 (Probability for Computer Scientists), Winter 2016</p></li>
        <li><p class="text-left">Teaching Assistant, CS 131 (Computer Vision: Foundations and Applications), Fall 2015</p></li>
      </ul>
    </div>
  </div>
</section>

<!-- Services Section -->
<section id="services" class="services-section">
  <div class="container">
    <div class="row">
      <h1 class="subtitle">Selected Publications</h1>
    </div>

    <div class="row">
      <h3 class="text-left">Computer Vision and Deep Learning</h3>
      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/luo2017label.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Label Efficient Learning of Transferable Representations across Domains and Tasks</h4>
          <p class="text-left">
            <span class="label label-success">Domain Adaptation</span>
            <span class="label label-success">Semi-Supervised Learning</span>
            <span class="label label-success">Transfer Learning</span>
          </p>
          <p class="text-left"><strong>Zelun Luo</strong>, Yuliang Zou, Judy Hoffman, Li Fei-Fei</p>
          <p class="text-left">Conference on Neural Information Processing Systems (NIPS) 2017</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content=
               "We propose a framework that learns a representation transferable across different domains and tasks in a data efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/nips17_website/luo_nips17.pdf">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/nips17_website/">Project</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/luo2017unsupervised.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Unsupervised Learning of Long-Term Motion Dynamics for Videos</h4>
          <p class="text-left">
            <span class="label label-success">Action Recognition</span>
            <span class="label label-success">Unsupervised Learning</span>
            <span class="label label-success">3D Computer Vision</span>
          </p>
          <p class="text-left"><strong>Zelun Luo</strong>, Boya Peng, De-An Huang, Alexandre Alahi, Li Fei-Fei</p>
          <p class="text-left">Conference on Computer Vision and Pattern Recognition (CVPR) 2017</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="We present an unsupervised representation learning approach
                                              that compactly encodes the motion dependencies
                                              in videos. Given a pair of images from a video clip, our
                                              framework learns to predict the long-term 3D motions. To
                                              reduce the complexity of the learning framework, we propose
                                              to describe the motion as a sequence of atomic 3D
                                              flows computed with RGB-D modality. We use a Recurrent
                                              Neural Network based Encoder-Decoder framework
                                              to predict these sequences of flows. We argue that in order
                                              for the decoder to reconstruct these sequences, the encoder
                                              must learn a robust video representation that captures
                                              long-term motion dependencies and spatial-temporal relations.
                                              We demonstrate the effectiveness of our learned temporal
                                              representations on activity classification across multiple
                                              modalities and datasets such as NTU RGB+D and MSR
                                              Daily Activity 3D. Our framework is generic to any input
                                              modality, i.e., RGB, depth, and RGB-D videos.">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1701.01821.pdf">PDF</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/haque2016towards.gif" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Towards Viewpoint Invariant 3D Human Pose Estimation</h4>
          <p class="text-left">
            <span class="label label-success">Human Pose Estimation</span>
            <span class="label label-success">3D Computer Vision</span>
          </p>
          <p class="text-left">Albert Haque, <strong>Zelun Luo*</strong>, Boya Peng*, Alexandre Alahi, Serena Yeung, Li Fei-Fei</p>
          <p class="text-left">European Conference on Computer Vision (ECCV) 2016</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="We propose a viewpoint invariant model for 3D human pose
                                              estimation from a single depth image. To achieve this, our discriminative
                                              model embeds local regions into a learned viewpoint invariant feature
                                              space. Formulated as a multi-task learning problem, our model is able to
                                              selectively predict partial poses in the presence of noise and occlusion.
                                              Our approach leverages a convolutional and recurrent network architecture
                                              with a top-down error feedback mechanism to self-correct previous
                                              pose estimates in an end-to-end manner. We evaluate our model on a
                                              previously published depth dataset and a newly collected human pose
                                              dataset containing 100K annotated depth images from extreme viewpoints.
                                              Experiments show that our model achieves competitive performance
                                              on frontal views while achieving state-of-the-art performance on
                                              alternate viewpoints.">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1603.07076.pdf">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://www.albert.cm/projects/viewpoint_3d_pose/">Website</a>
          </div>
        </div>
      </div>
    </div>

    <div class="row">
      <h3 class="text-left">AI + Healthcare</h3>
      <p class="text-left">
        I am a member of the Stanford Program in AI-Assisted Care (PAC), which is a collaboration between the Stanford AI Lab and Stanford Clinical Excellence Research Center that aims to use computer vision and machine learning to create AI-assisted smart healthcare spaces. 
      </p>
      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/haque2017towards.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene Compliance</h4>
          <p class="text-left">
            <span class="label label-success">Action Recognition</span>
            <span class="label label-success">3D Computer Vision</span>
          </p>
          <p class="text-left">Albert Haque, Michelle Guo, Alexandre Alahi, Serena Yeung, <strong>Zelun Luo</strong>, Alisha Rege, Amit Singh, Jeffrey Jopling, Lance Downing, William Beninati, Terry Platchek, Arnold Milstein, Li Fei-Fei</p>
          <p class="text-left">Machine Learning for Healthcare (MLHC) 2017</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="One in twenty-five patients admitted to a hospital will suffer from a hospital acquired infection. If we can intelligently track healthcare staff, patients, and visitors, we can better understand the sources of such infections. We envision a smart hospital capable of increasing operational efficiency and improving patient care with less spending. In this paper, we propose a non-intrusive vision-based system for tracking people’s activity in hospitals. We evaluate our method for the problem of measuring hand hygiene compliance. Empirically, our method outperforms existing solutions such as proximity-based techniques and covert in-person observational studies. We present intuitive, qualitative results that analyze human movement patterns and conduct spatial analytics which convey our method’s interpretability. This work is a first step towards a computer-vision based smart hospital and demonstrates promising results for reducing hospital acquired infections.">Abstract</a>
               <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1708.00163.pdf">PDF</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/luo2017computer.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Computer Vision-based Approach to Maintain Independent Living for Seniors</h4>
          <p class="text-left">
            <span class="label label-success">Action Recognition</span>
            <span class="label label-success">3D Computer Vision</span>
          </p>
          <p class="text-left"><strong>Zelun Luo</strong>, Alisha Rege, Guido Pusiol, Arnold Milstein, Li Fei-Fei, N. Lance Downing</p>
          <p class="text-left">American Medical Informatics Association (AMIA) 2017</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content=
               "Recent progress in developing cost-effective sensors and machine learning techniques has enabled new AI-assisted solutions for human behavior understanding. In this work, we investigate the use of thermal and depth sensors for the detection of daily activities, lifestyle patterns, emotions, and vital signs, as well as the development of intelligent mechanisms for accurate situational assessment and rapid response. We demonstrate an integrated solution for remote monitoring, assessment, and support of seniors living independently at home."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="publications/AMIA-Poster.pdf">Poster</a>
            <a class="btn btn-default btn-xs" target="_blank" href="publications/luo2017computer.pdf">Manuscript</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/yeung2015vision.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Vision-Based Hand Hygiene Monitoring in Hospitals</h4>
          <p class="text-left">
            <span class="label label-success">Action Recognition</span>
            <span class="label label-success">3D Computer Vision</span>
          </p>
          <p class="text-left">Serena Yeung, Alexandre Alahi, <strong>Zelun Luo</strong>, Boya Peng, Albert Haque, Amit Singh, Terry Platchek,
            Arnold Milstein, Li Fei-Fei</p>
          <p class="text-left">NIPS 2015 Machine Learning for Health Care Workshop</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="Recent progress in developing cost-effective depth sensors has enabled new AI-assisted
                                              solutions such as assisted driving vehicles and smart spaces. Machine
                                              learning techniques have been successfully applied on these depth signals to perceive
                                              meaningful information about human behavior. In this work, we propose
                                              to deploy depth sensors in hospital settings and use computer vision methods to
                                              enable AI-assisted care. We aim to reduce visually-identifiable human errors such
                                              as hand hygiene compliance, one of the leading causes of Health Care-Associated
                                              Infection (HCAI) in hospitals."
               >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://ai.stanford.edu/~syyeung/resources/vision_hand_hh_nipsmlhc.pdf">PDF</a>
          </div>
        </div>
      </div>
    </div>

    <div class="row">
      <h3 class="text-left">Biomedical Imaging and Diagnosis</h3>
      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/kandel2017label.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Label-Free Tissue Scanner for Colorectal Cancer Screening</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Mikhail E. Kandel, Shamira Sridharan, Jon Liang, <strong>Zelun Luo</strong>, Kevin Han, Virgilia Macias, Anish Shah,
            Roshan Patel, Krishnarao Tangella, Andre Kajdacsy-Balla, Grace Guzman, Gabriel Popescu</p>
          <p class="text-left">Journal of Biomedical Optics 2017</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="The current practice of surgical pathology relies on external contrast
                                              agents to reveal tissue architecture, which is then qualitatively examined
                                              by a trained pathologist. The diagnosis is based on the comparison with
                                              standardized empirical, qualitative assessments of limited objectivity.
                                              We propose an approach to pathology based on interferometric imaging of
                                              “unstained” biopsies, which provides unique capabilities for quantitative
                                              diagnosis and automation. We developed a label-free tissue scanner based
                                              on “quantitative phase imaging,” which maps out optical path length at
                                              each point in the field of view and, thus, yields images that are sensitive
                                              to the “nanoscale” tissue architecture. Unlike analysis of stained tissue,
                                              which is qualitative in nature and affected by color balance, staining
                                              strength and imaging conditions, optical path length measurements are
                                              intrinsically quantitative, i.e., images can be compared across different
                                              instruments and clinical sites. These critical features allow us to automate
                                              the diagnosis process. We paired our interferometric optical system with highly
                                              parallelized, dedicated software algorithms for data acquisition, allowing us
                                              to image at a throughput comparable to that of commercial tissue scanners while
                                              maintaining the nanoscale sensitivity to morphology. Based on the measured phase
                                              information, we implemented software tools for autofocusing during imaging, as
                                              well as image archiving and data access. To illustrate the potential of our
                                              technology for large volume pathology screening, we established an “intrinsic
                                              marker” for colorectal disease that detects tissue with dysplasia or colorectal
                                              cancer and flags specific areas for further examination, potentially improving
                                              the efficiency of existing pathology workflows. ">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://www.spiedigitallibrary.org/journals/Journal_of_Biomedical_Optics/volume-22/issue-6/066016/Label-free-tissue-scanner-for-colorectal-cancer-screening/10.1117/1.JBO.22.6.066016.full">PDF & Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/majeed2016towards.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Towards Quantitative Automated Histopathology of Breast Cancer using Spatial Light Interference Microscopy
            (SLIM)</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Hassaan Majeed, Tan H Nguyen, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias,
            Krishnarao Tangella, Andre Balla, Minh N Do, Gabriel Popescu</p>
          <p class="text-left">United States and Canadian Academy of Pathology (USCAP) 2016</p>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/majeed2015breast.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Breast Cancer Diagnosis using Spatial Light Interference Microscopy</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Hassaan Majeed, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias, Krishnarao Tangella,
            Andre Balla, Gabriel Popescu</p>
          <p class="text-left">Journal of Biomedical Optics 2015</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content=
               "The standard practice in histopathology of breast cancers is to examine a hematoxylin and eosin (H&E) stained tissue biopsy under a microscope to diagnose whether a lesion is benign or malignant. This determination is made based on a manual, qualitative inspection, making it subject to investigator bias and resulting in low throughput. Hence, a quantitative, label-free, and high-throughput diagnosis method is highly desirable. We present here preliminary results showing the potential of quantitative phase imaging for breast cancer screening and help with differential diagnosis. We generated phase maps of unstained breast tissue biopsies using spatial light interference microscopy (SLIM). As a first step toward quantitative diagnosis based on SLIM, we carried out a qualitative evaluation of our label-free images. These images were shown to two pathologists who classified each case as either benign or malignant. This diagnosis was then compared against the diagnosis of the two pathologists on corresponding H&E stained tissue images and the number of agreements were counted. The agreement between SLIM and H&E based diagnosis was 88% for the first pathologist and 87% for the second. Our results demonstrate the potential and promise of SLIM for quantitative, label-free, and high-throughput diagnosis."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank"
               href="http://light.ece.illinois.edu/wp-content/uploads/2015/10/Hassaan_JBO_20_11_111210.pdf">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank"
               href="http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=2430724">Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/majeed2015high.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">High Throughput Imaging of Blood Smears using White Light Diffraction Phase Microscopy</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Hassaan Majeed, Mikhail E Kandel, Basanta Bhaduri, Kevin Han, <strong>Zelun Luo</strong>, Krishnarao Tangella,
            Gabriel Popescu</p>
          <p class="text-left">SPIE Photonics West: BiOS 2015</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content=
               "While automated blood cell counters have made great progress in detecting abnormalities in blood, the lack of specificity for a particular disease, limited information on single cell morphology and intrinsic uncertainly due to high throughput in these instruments often necessitates detailed inspection in the form of a peripheral blood smear. Such tests are relatively time consuming and frequently rely on medical professionals tally counting specific cell types. These assays rely on the contrast generated by chemical stains, with the signal intensity strongly related to staining and preparation techniques, frustrating machine learning algorithms that require consistent quantities to denote the features in question. Instead we opt to use quantitative phase imaging, understanding that the resulting image is entirely due to the structure (intrinsic contrast) rather than the complex interplay of stain and sample. We present here our first steps to automate peripheral blood smear scanning, in particular a method to generate the quantitative phase image of an entire blood smear at high throughput using white light diffraction phase microscopy (wDPM), a single shot and common path interferometric imaging technique."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204110">
              PDF & Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/majeed2015diagnosis.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Diagnosis of Breast Cancer Biopsies using Quantitative Phase Imaging</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Hassaan Majeed, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias, Krishnarao Tangella,
            Andre Balla, Gabriel Popescu</p>
          <p class="text-left">SPIE Photonics West: BiOS 2015</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content=
               "The standard practice in the histopathology of breast cancers is to examine a hematoxylin and eosin (H&E) stained tissue biopsy under a microscope. The pathologist looks at certain morphological features, visible under the stain, to diagnose whether a tumor is benign or malignant. This determination is made based on qualitative inspection making it subject to investigator bias. Furthermore, since this method requires a microscopic examination by the pathologist it suffers from low throughput. A quantitative, label-free and high throughput method for detection of these morphological features from images of tissue biopsies is, hence, highly desirable as it would assist the pathologist in making a quicker and more accurate diagnosis of cancers. We present here preliminary results showing the potential of using quantitative phase imaging for breast cancer screening and help with differential diagnosis. We generated optical path length maps of unstained breast tissue biopsies using Spatial Light Interference Microscopy (SLIM). As a first step towards diagnosis based on quantitative phase imaging, we carried out a qualitative evaluation of the imaging resolution and contrast of our label-free phase images. These images were shown to two pathologists who marked the tumors present in tissue as either benign or malignant. This diagnosis was then compared against the diagnosis of the two pathologists on H&E stained tissue images and the number of agreements were counted. In our experiment, the agreement between SLIM and H&E based diagnosis was measured to be 88%. Our preliminary results demonstrate the potential and promise of SLIM for a push in the future towards quantitative, label-free and high throughput diagnosis."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204106">
              PDF & Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="publications/kandel2015cpp.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">C++ Software Integration for a High-throughput Phase Imaging Platform</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Mikhail E Kandel, <strong>Zelun Luo</strong>, Kevin Han, Gabriel Popescu</p>
          <p class="text-left">SPIE Photonics West: BiOS 2015</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               data-html="True" title="Abstract" data-content=
               "The multi-shot approach in SLIM requires reliable, synchronous, and parallel operation of three independent hardware devices – not meeting these challenges results in degraded phase and slow acquisition speeds, narrowing applications to holistic statements about complex phenomena. The relative youth of quantitative imaging and the lack of ready-made commercial hardware and tools further compounds the problem as Higher level programming languages result in inflexible, experiment specific instruments limited by ill-fitting computational modules, resulting in a palpable chasm between promised and realized hardware performance. Furthermore, general unfamiliarity with intricacies such as background calibration, objective lens attenuation, along with spatial light modular alignment, makes successful measurements difficult for the inattentive or uninitiated. This poses an immediate challenge for moving our techniques beyond the lab to biologically oriented collaborators and clinical practitioners.
                <br />To meet these challenges, we present our new Quantitative Phase Imaging pipeline, with improved instrument performance, friendly user interface and robust data processing features, enabling us to acquire and catalog clinical datasets hundreds of gigapixels in size."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204094">
              PDF & Website</a>
          </div>
        </div>
      </div>
    </div>


  </div>
</section>

<!-- Contact Section -->
<section id="contact" class="contact-section">
  <div class="container">
    <div class="row">
      <div class="col-sm-12">
        <h1 class="subtitle">Projects</h1>
      </div>
    </div>

    <div class="row">
      <div class="row paper">
        <div class="col-sm-3">
          <img src="projects/text.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">End-to-End Deep Neural Network for Text Recognition in the Wild</h4>
          <p class="text-left">
            <span class="label label-success">Scene Text Recognition</span>
          </p>
          <p class="text-left"><strong>Zelun Luo</strong></p>
          <p class="text-left">Research Project at Amazon A9</p>
          <p class="text-left">Advisors: Dr. Son Tran, Dr. R. Manmatha</p>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="projects/captioning.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Show, Discriminate, and Tell: A Discriminative Image Captioning Model with Deep Neural Networks</h4>
          <p class="text-left">
            <span class="label label-success">Image Captioning</span>
          </p>
          <p class="text-left"><strong>Zelun Luo*</strong>, Boya Peng*, Te-Lin Wu*</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               data-html="True" title="Abstract" data-content="Caption generation has long been seen as a difficult
                                                               problem in Computer Vision and Natural Language Processing.
                                                               In this paper, we present an image captioning
                                                               model based on a end-to-end neural framework that combines
                                                               Convolutional Neural Network and Recurrent Neural
                                                               Network. Critical to our approach is a ranking objective
                                                               that attempts to add discriminatory power to the model. Experiments
                                                               on MS COCO dataset shows that our model consistently
                                                               outperforms its counterpart with no ranking objective,
                                                               both quantitatively based on BLEU and CIDEr scores
                                                               and qualitatively based on human evaluation.">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://web.stanford.edu/class/cs231a/prev_projects_2016/cs231a.pdf">PDF</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="projects/navi.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Navi: Your In-Store Navigator</h4>
          <p class="text-left">
            <span class="label label-success">Augmented Reality</span>
            <span class="label label-success">Google Tango (Android)</span>
            <span class="label label-success">SLAM</span>
            <span class="label label-success">Startup Project</span>
          </p>
          <p class="text-left"><strong>Zelun Luo*</strong>, Genie Hyatt*, Zuozhen Liu*, Catherine Mullings*, Boya Peng*</p>
          <div class="text-left">
            <a class="btn btn-default btn-xs" target="_blank" href="projects/navi1.pdf">Report</a>
            <a class="btn btn-default btn-xs" target="_blank" href="projects/navi2.pdf">Interface</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://www.youtube.com/watch?v=FNEQI18o1J8">Demo</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="projects/deep-annotator.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Deep Annotator: A General Purpose Web Annotation Interface</h4>
          <p class="text-left">
            <span class="label label-success">Web Development</span>
            <span class="label label-success">React</span>
          </p>
          <p class="text-left"><strong>Zelun Luo*</strong>, Boya Peng*</p>
          <div class="text-left">
            <a class="btn btn-default btn-xs" target="_blank" href="https://github.com/d1ngn1gefe1/deepannotator">Code</a>
          </div>
        </div>
      </div>

      <h4 class="text-left">2014</h4>

      <div class="row paper">
        <div class="col-sm-3">
          <img src="projects/superstitchous.png" class="paper-fig img-responsive">
        </div>
        <div class="col-sm-8 col-sm-offset-1">
          <h4 class="paper-title text-left">Superstitchous: Image Stitching and Core Segmentation Software for Large Scale Digital Holography</h4>
          <p class="text-left">
            <span class="label label-success">Image Stitching</span>
            <span class="label label-success">Image Segmentation</span>
            <span class="label label-success">Web Development</span>
          </p>
          <p class="text-left"><strong>Zelun Luo*</strong>, Kevin Han*</p>
          <p class="text-left">Senior Design</p>
          <p class="text-left">Advisor: Prof Gabriel Popescu</p>
          <div class="text-left">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               data-html="True" title="Abstract" data-content=
               "We implemented an image stitching program for quantitative phase microscope images, as well as a  web-based image viewer and core segmentation program. The image stitching program is able to stitch  entire slides consisting of hundreds of gigabytes of data, and the user can upload them to our server and  view them remotely. Zooming and panning of images is supported. Finally, biopsy cores in the image can  be automatically identified and output into separate images."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="projects/senior_design.pdf">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://github.com/d1ngn1gefe1/superstitchous2.0">Code</a>
          </div>
        </div>
      </div>
    </div>

  </div>
</section>

<!--Google Analytics-->
<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
    a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-100690611-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- jQuery -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"
        integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Custom JavaScript -->
<script src="js/effects.js"></script>

</body>

</html>
