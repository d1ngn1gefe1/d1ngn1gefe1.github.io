<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Zelun (Alan) Luo</title>

  <!-- Latest compiled and minified CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
        integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  <!-- Custom CSS -->
  <link href="sass/style.css" rel="stylesheet">
</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap
scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

<!-- Navigation -->
<nav class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header page-scroll">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand page-scroll" href="#page-top">Zelun (Alan) Luo</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse navbar-ex1-collapse">
      <ul class="nav navbar-nav">
        <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
        <li class="hidden">
          <a class="page-scroll" href="#page-top"></a>
        </li>
        <li>
          <a class="page-scroll" href="#about">About</a>
        </li>
        <li>
          <a class="page-scroll" href="#services">Publications</a>
        </li>
        <li>
          <a class="page-scroll" href="#contact">Projects</a>
        </li>
      </ul>
    </div>
    <!-- /.navbar-collapse -->
  </div>
  <!-- /.container -->
</nav>

<!-- Intro Section -->
<section id="intro" class="intro-section">
  <div class="container">
    <div class="row">
      <div class="col-xs-6 col-xs-offset-3 my-info">
        <h1>Zelun (Alan) Luo</h1>
        <p><br>Stanford Vision and Learning Lab<br>Advisor: Prof. Fei-Fei Li<br>Stanford University<br><br>zelunluo at stanford dot edu</p>
        <p>
          <a href="https://scholar.google.com/citations?user=y90O9pUAAAAJ&hl=en">[Google Scholar]</a>
          <a href="https://github.com/d1ngn1gefe1">[Github]</a>
          <a href="intro/cv.pdf">[CV (updated on Dec 2017)]</a>
        </p>
      </div>
    </div>
  </div>
</section>

<!-- About Section -->
<section id="about" class="about-section">
  <div class="container">
    <div class="row">
      <h1 class="subtitle">About</h1>
    </div>

    <div class="row">
      <p class="text-left">
        I am an incoming PhD student in the Computer Science department at Stanford University starting in September 2018. I am working in the <a href="http://vision.stanford.edu/">Stanford Vision and Learning Lab</a>, advised by <a href="http://vision.stanford.edu/feifeili/">Prof. Fei-Fei Li</a>.
      </p>
      <p class="text-left">
        I am a member of the <a href="https://aicare.stanford.edu/">Stanford Program in AI-Assisted Care (PAC)</a>, which is a collaboration between the Stanford AI Lab and Stanford Clinical Excellence Research Center that aims to use computer vision and machine learning to create AI-assisted smart healthcare spaces.
      </p>
      <p class="text-left">
        Before that, I received my master's degree from Stanford University and my bachelor's degree from the University of Illinois Urbana-Champaign. During my master's study, I spent three years researching in the <a href="http://vision.stanford.edu/">Stanford Vision and Learning Lab</a> with <a href="http://vision.stanford.edu/feifeili/">Prof. Fei-Fei Li</a>, and the <a href="http://med.stanford.edu/cerc.html">Clinical Excellence Research Center</a> with <a href="https://med.stanford.edu/profiles/arnold-milstein">Prof. Arnold Milstein</a>. During my undergraduate study, I spent three years reseaching in the <a href="http://light.ece.illinois.edu/">Quantitative Light Imaging Laboratory</a> with <a href="http://light.ece.illinois.edu/">Prof. Gabriel Popescu</a>, and the <a href="http://vision.ai.illinois.edu/">Computer Vision and Robotics Laboratory</a> with <a href="https://filebox.ece.vt.edu/~jbhuang/">Prof. Jia-Bin Huang</a> and <a href="http://vision.ai.illinois.edu/ahuja.html">Prof. Narendra Ahuja</a>.
      </p>
    </div>


    <div class="row">
      <div class="col-xs-8">
        <h3 class="text-left">News</h3>
        <div class="news">
          <ul>
            <li><p class="text-left">[2018/07] Two papers were accepted to ECCV 2018. </p></li>
            <li><p class="text-left">[2018/06] Our work on senior healthcare was accepted to MLHC 2018. </p></li>
            <li><p class="text-left">[2018/03] I accepted the PhD offer from Stanford University. </p></li>
            <li><p class="text-left">[2017/12] Two papers are available on arXiv! <a href="https://arxiv.org/abs/1712.00108">[Graph Distillation]</a> <a href="https://arxiv.org/abs/1712.00123">[Label Efficient Learning]</a>  </p></li>
            <li><p class="text-left">[2017/11] I am applying to PhD programs for Autumn 2018 entry. </p></li>
            <li><p class="text-left">[2017/11] I will be an official reviewer for CVPR 2018.</p></li>
            <li><p class="text-left">[2017/11] I am presenting our work on <a href="http://alan.vision/publications/AMIA-Poster.pdf">AI-assisted Senior Healthcare</a> at <a href="https://www.amia.org/amia2017">AMIA 2017</a> in Washington DC.</p></li>
            <li><p class="text-left">[2017/09] Our paper, <a href="http://alan.vision/nips17_website/">Label Efficient Learning of Transferable Representations across Domains and Tasks</a>, is accepted to <a href="https://nips.cc/">NIPS 2017</a>. See you at Long Beach in December!</p></li>
            <li><p class="text-left">[2017/07] I presented my work, <a href="https://arxiv.org/pdf/1701.01821.pdf">Unsupervised Learning of Long-Term Motion Dynamics for Videos</a>, at <a href="http://cvpr2017.thecvf.com/">CVPR 2017</a> in Honolulu, Hawaii.</p></li>
            <li><p class="text-left">[2017/06] I am lucky to be an intern at Google with <a href="http://www.cs.cmu.edu/~lujiang/">Dr. Lu Jiang</a> and <a href="http://vision.stanford.edu/lijiali/">Dr. Jia Li</a>.</p></li>
          </ul>
        </div>
      </div>
    </div>


    <div class="row">
      <div class="col-xs-10">
        <h3 class="text-left">Education</h3>
        <div class="col-xs-4">
          <img src="about/stanford_seal.png" class="school-fig">
          <p>Doctor of Philosophy<br>
            Computer Science<br>
            Stanford University<br>
            2018 - Present
          </p>
        </div>
        <div class="col-xs-4">
          <img src="about/stanford_seal.png" class="school-fig">
          <p>Master of Science<br>
            Computer Science<br>
            Stanford University<br>
            2015 - 2018
          </p>
        </div>
        <div class="col-xs-4">
          <img src="about/uiuc_seal.png" class="school-fig">
          <p>Bachelor of Science<br>
            Electrical and Computer Engineering<br>
            University of Illinois at Urbana-Champaign<br>
            2012 - 2015
          </p>
        </div>
      </div>
    </div>

    <div class="row">
      <div class="col-xs-10">
        <h3 class="text-left">Industry</h3>
        <div class="col-xs-4">
          <img src="about/google.png" class="company-fig img-circle">
          <p>Google<br>
            Research Intern<br>
            June 2017 - Present
          </p>
        </div>
        <div class="col-xs-4">
          <img src="about/a9.png" class="company-fig img-circle">
          <p>Amazon A9<br>
            Research Intern<br>
            June 2016 - September 2016
          </p>
        </div>
        <div class="col-xs-4">
          <img src="about/yahoo.png" class="company-fig img-circle">
          <p>Yahoo<br>
            Software Engineering Intern<br>
            May 2015 - September 2015
          </p>
        </div>
      </div>
    </div>

    <div class="row">
      <h3 class="text-left">Teaching</h3>
      <ul>
        <li><p class="text-left">Teaching Assistant, CS 231N (Convolutional Neural Networks for Visual Recognition), Spring 2017</p></li>
        <li><p class="text-left">Teaching Assistant, CS 224N (Natural Language Processing with Deep Learning), Winter 2017</p></li>
        <li><p class="text-left">Head Teaching Assistant, CS 131 (Computer Vision: Foundations and Applications), Fall 2016</p></li>
        <li><p class="text-left">Teaching Assistant, CS 109 (Probability for Computer Scientists), Spring 2016</p></li>
        <li><p class="text-left">Teaching Assistant, CS 109 (Probability for Computer Scientists), Winter 2016</p></li>
        <li><p class="text-left">Teaching Assistant, CS 131 (Computer Vision: Foundations and Applications), Fall 2015</p></li>
      </ul>
    </div>
  </div>
</section>

<!-- Services Section -->
<section id="services" class="services-section">
  <div class="container">
    <div class="row">
      <h1 class="subtitle">Selected Publications</h1>
    </div>

    <div class="row">
      <h3 class="text-left">Computer Vision and Deep Learning</h3>
      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/luo2018graph.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Graph Distillation for Action Detection with Privileged Information</h4>
          <p class="text-left">
            <span class="label label-success">Action Recognition</span>
            <span class="label label-success">Multi-modal Learning</span>
            <span class="label label-success">Knowledge Distillation</span>
            <span class="label label-success">Learning Using Priviledged Information</span>
          </p>
          <p class="text-left"><strong>Zelun Luo</strong>, Jun-Ting Hsieh, Lu Jiang, Juan Carlos Niebles, Li Fei-Fei</p>
          <p class="text-left">European Conference on Computer Vision (ECCV) 2018</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="In this work, we propose a technique that tackles the video understanding problem under a realistic, demanding condition in which we have limited labeled data and partially observed training modalities. Common methods such as transfer learning do not take advantage of the rich information from extra modalities potentially available in the source domain dataset. On the other hand, previous work on cross-modality learning only focuses on a single domain or task. In this work, we propose a graph-based distillation method that incorporates rich privileged information from a large multi-modal dataset in the source domain, and shows an improved performance in the target domain where data is scarce. Leveraging both a large-scale dataset and its extra modalities, our method learns a better model for temporal action detection and action classification without needing to have access to these modalities during test time. We evaluate our approach on action classification and temporal action detection tasks, and show that our models achieve the state-of-the-art performance on the PKU-MMD and NTU RGB+D datasets."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/1712.00108">PDF</a>
            <a class="btn btn-default btn-xs disabled" target="_blank" href="http://alan.vision/">Project</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/zou2018dfnet.gif" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Network Consistency</h4>
          <p class="text-left">
            <span class="label label-success">Unsupervised Learning</span>
            <span class="label label-success">Domain Adaptation</span>
            <span class="label label-success">Depth Estimation</span>
            <span class="label label-success">Flow Estimation</span>
          </p>
          <p class="text-left">Yuliang Zou, <strong>Zelun Luo</strong>, Jia-Bin Huang</p>
          <p class="text-left">European Conference on Computer Vision (ECCV) 2018</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="We present an unsupervised adaptation framework for simultaneously improving single-view depth estimation and optical flow estimation models in unseen scenes using unlabeled videos. Supervised adaptation approaches require costly dense pixelwise ground truth for fine-tuning the models in the target domain. Whilst unsupervised adaptation techniques exploiting brightness constancy and spatial smoothness priors have been proposed, these assumptions often do not hold and hence makes the training unstable. In this paper, we propose to leverage geometric consistency as a supervisory signal for adapting both depth and flow estimation models in new domains. Our core idea is that for rigid regions we can use the predicted scene depth and camera motion for synthesizing 2D optical flow by backprojecting the induced 3D scene flow. The discrepancy between the synthesized flow (from depth prediction and camera motion) and the estimated flow (from optical flow prediction model) allows us to impose a cross-model consistency loss. All the networks are jointly optimized during training. However, individual models can be applied independently at test time. Extensive experiments on both tasks demonstrate the effectiveness of our approach."
            >Abstract</a>
            <a class="btn btn-default btn-xs disabled" target="_blank" href="http://alan.vision/">PDF</a>
            <a class="btn btn-default btn-xs disabled" target="_blank" href="http://alan.vision/">Project</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/luo2017label.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Label Efficient Learning of Transferable Representations across Domains and Tasks</h4>
          <p class="text-left">
            <span class="label label-success">Domain Adaptation</span>
            <span class="label label-success">Semi-Supervised Learning</span>
            <span class="label label-success">Transfer Learning</span>
          </p>
          <p class="text-left"><strong>Zelun Luo</strong>, Yuliang Zou, Judy Hoffman, Li Fei-Fei</p>
          <p class="text-left">Conference on Neural Information Processing Systems (NIPS) 2017</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content=
               "We propose a framework that learns a representation transferable across different domains and tasks in a data efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/1712.00123">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/nips17_website/poster.pdf">Poster</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/nips17_website/">Project</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/luo2017unsupervised.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Unsupervised Learning of Long-Term Motion Dynamics for Videos</h4>
          <p class="text-left">
            <span class="label label-success">Action Recognition</span>
            <span class="label label-success">Unsupervised Learning</span>
            <span class="label label-success">3D Computer Vision</span>
          </p>
          <p class="text-left"><strong>Zelun Luo</strong>, Boya Peng, De-An Huang, Alexandre Alahi, Li Fei-Fei</p>
          <p class="text-left">Conference on Computer Vision and Pattern Recognition (CVPR) 2017</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="We present an unsupervised representation learning approach
                                              that compactly encodes the motion dependencies
                                              in videos. Given a pair of images from a video clip, our
                                              framework learns to predict the long-term 3D motions. To
                                              reduce the complexity of the learning framework, we propose
                                              to describe the motion as a sequence of atomic 3D
                                              flows computed with RGB-D modality. We use a Recurrent
                                              Neural Network based Encoder-Decoder framework
                                              to predict these sequences of flows. We argue that in order
                                              for the decoder to reconstruct these sequences, the encoder
                                              must learn a robust video representation that captures
                                              long-term motion dependencies and spatial-temporal relations.
                                              We demonstrate the effectiveness of our learned temporal
                                              representations on activity classification across multiple
                                              modalities and datasets such as NTU RGB+D and MSR
                                              Daily Activity 3D. Our framework is generic to any input
                                              modality, i.e., RGB, depth, and RGB-D videos.">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1701.01821.pdf">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="publications/CVPR2017-poster.png">Poster</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/haque2016towards.gif" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Towards Viewpoint Invariant 3D Human Pose Estimation</h4>
          <p class="text-left">
            <span class="label label-success">Human Pose Estimation</span>
            <span class="label label-success">3D Computer Vision</span>
          </p>
          <p class="text-left">Albert Haque, <strong>Zelun Luo*</strong>, Boya Peng*, Alexandre Alahi, Serena Yeung, Li Fei-Fei</p>
          <p class="text-left">European Conference on Computer Vision (ECCV) 2016</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="We propose a viewpoint invariant model for 3D human pose
                                              estimation from a single depth image. To achieve this, our discriminative
                                              model embeds local regions into a learned viewpoint invariant feature
                                              space. Formulated as a multi-task learning problem, our model is able to
                                              selectively predict partial poses in the presence of noise and occlusion.
                                              Our approach leverages a convolutional and recurrent network architecture
                                              with a top-down error feedback mechanism to self-correct previous
                                              pose estimates in an end-to-end manner. We evaluate our model on a
                                              previously published depth dataset and a newly collected human pose
                                              dataset containing 100K annotated depth images from extreme viewpoints.
                                              Experiments show that our model achieves competitive performance
                                              on frontal views while achieving state-of-the-art performance on
                                              alternate viewpoints.">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1603.07076.pdf">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://www.albert.cm/projects/viewpoint_3d_pose/">Website</a>
          </div>
        </div>
      </div>
    </div>

    <div class="row">
      <h3 class="text-left">AI-Assisted Healthcare</h3>
      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/luo2018computer.gif" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Computer Vision-based Descriptive Analytics of Seniors' Daily Activities for Long-term Health Monitoring</h4>
          <p class="text-left">
            <span class="label label-success">Action Detection</span>
            <span class="label label-success">Multi-modal Learning</span>
          </p>
          <p class="text-left"><strong>Zelun Luo*</strong>, Jun-Ting Hsieh*, Niranjan Balachandar, Serena Yeung, Guido Pusiol, Jay Luxenberg, Grace Li, Li-Jia Li, Lance Downing, Arnold Milstein, Li Fei-Fei</p>
          <p class="text-left">Machine Learning for Healthcare (MLHC) 2018, Stanford, CA, August 17-18, 2018</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="One in twenty-five patients admitted to a hospital will suffer from a hospital acquired infection. If we can intelligently track healthcare staff, patients, and visitors, we can better understand the sources of such infections. We envision a smart hospital capable of increasing operational efficiency and improving patient care with less spending. In this paper, we propose a non-intrusive vision-based system for tracking people’s activity in hospitals. We evaluate our method for the problem of measuring hand hygiene compliance. Empirically, our method outperforms existing solutions such as proximity-based techniques and covert in-person observational studies. We present intuitive, qualitative results that analyze human movement patterns and conduct spatial analytics which convey our method’s interpretability. This work is a first step towards a computer-vision based smart hospital and demonstrates promising results for reducing hospital acquired infections."
            >Abstract</a>
            <a class="btn btn-default btn-xs disabled" target="_blank" href="http://alan.vision/">PDF</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/haque2017towards.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene Compliance</h4>
          <p class="text-left">
            <span class="label label-success">Action Recognition</span>
            <span class="label label-success">3D Computer Vision</span>
          </p>
          <p class="text-left">Albert Haque, Michelle Guo, Alexandre Alahi, Serena Yeung, <strong>Zelun Luo</strong>, Alisha Rege, Amit Singh, Jeffrey Jopling, Lance Downing, William Beninati, Terry Platchek, Arnold Milstein, Li Fei-Fei</p>
          <p class="text-left">Machine Learning for Healthcare (MLHC) 2017, Boston, MA, August 18-19, 2017</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="Nations around the world face rising demand for costly long-term care for seniors. Patterns
                                              in seniors' activities of daily living, such as sleeping, sitting, standing, walking, etc. can
                                              provide caregivers useful clues regarding seniors' health. As the senior population continues
                                              to grow worldwide, continuous manual monitoring of seniors' daily activities will
                                              become more and more challenging for caregivers. Thus to improve caregivers' ability
                                              to assist seniors, an automated system for monitoring and analyzing patterns in seniors
                                              activities of daily living would be useful. A possible approach to implementing such a
                                              system involves wearable sensors, but this approach is intrusive and requires adherence by
                                              patients. In this paper, using a dataset we collected from an assisted-living facility for
                                              seniors, we present a novel computer vision-based approach that leverages nonintrusive,
                                              privacy-compliant multi-modal sensors and state-of-the-art computer vision techniques for
                                              continuous activity detection to remotely detect and provide long-term descriptive analytics
                                              of senior activities. These analytics include both qualitative and quantitative descriptions
                                              of senior daily activity patterns that can be interpreted by caregivers. Our work is progress
                                              towards a smart senior home that uses computer vision to support caregivers in senior
                                              healthcare to help meet the challenges of an aging worldwide population.">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1708.00163.pdf">PDF</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/luo2017computer.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Computer Vision-based Approach to Maintain Independent Living for Seniors</h4>
          <p class="text-left">
            <span class="label label-success">Action Recognition</span>
            <span class="label label-success">Multi-modal Learning</span>
          </p>
          <p class="text-left"><strong>Zelun Luo</strong>, Alisha Rege, Guido Pusiol, Arnold Milstein, Li Fei-Fei, N. Lance Downing</p>
          <p class="text-left">American Medical Informatics Association (AMIA), Washington, DC, November 4-8, 2017</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content=
               "Recent progress in developing cost-effective sensors and machine learning techniques has enabled new AI-assisted solutions for human behavior understanding. In this work, we investigate the use of thermal and depth sensors for the detection of daily activities, lifestyle patterns, emotions, and vital signs, as well as the development of intelligent mechanisms for accurate situational assessment and rapid response. We demonstrate an integrated solution for remote monitoring, assessment, and support of seniors living independently at home."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="publications/AMIA-Poster.pdf">Poster</a>
            <a class="btn btn-default btn-xs" target="_blank" href="publications/luo2017computer.pdf">Manuscript</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/yeung2015vision.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Vision-Based Hand Hygiene Monitoring in Hospitals</h4>
          <p class="text-left">
            <span class="label label-success">Action Recognition</span>
            <span class="label label-success">3D Computer Vision</span>
          </p>
          <p class="text-left">Serena Yeung, Alexandre Alahi, <strong>Zelun Luo</strong>, Boya Peng, Albert Haque, Amit Singh, Terry Platchek,
            Arnold Milstein, Li Fei-Fei</p>
          <p class="text-left">American Medical Informatics Association (AMIA), Chicago, November 12-16, 2016</p>
          <p class="text-left">NIPS Workshop on Machine Learning for Healthcare (MLHC), 2015</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="Recent progress in developing cost-effective depth sensors has enabled new AI-assisted
                                              solutions such as assisted driving vehicles and smart spaces. Machine
                                              learning techniques have been successfully applied on these depth signals to perceive
                                              meaningful information about human behavior. In this work, we propose
                                              to deploy depth sensors in hospital settings and use computer vision methods to
                                              enable AI-assisted care. We aim to reduce visually-identifiable human errors such
                                              as hand hygiene compliance, one of the leading causes of Health Care-Associated
                                              Infection (HCAI) in hospitals."
               >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://ai.stanford.edu/~syyeung/resources/vision_hand_hh_nipsmlhc.pdf">PDF</a>
          </div>
        </div>
      </div>
    </div>

    <div class="row">
      <h3 class="text-left">Biomedical Imaging and Diagnosis</h3>
      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/kandel2017label.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Label-Free Tissue Scanner for Colorectal Cancer Screening</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Mikhail E. Kandel, Shamira Sridharan, Jon Liang, <strong>Zelun Luo</strong>, Kevin Han, Virgilia Macias, Anish Shah,
            Roshan Patel, Krishnarao Tangella, Andre Kajdacsy-Balla, Grace Guzman, Gabriel Popescu</p>
          <p class="text-left">Journal of Biomedical Optics, Opt. 22(6), 2017</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content="The current practice of surgical pathology relies on external contrast
                                              agents to reveal tissue architecture, which is then qualitatively examined
                                              by a trained pathologist. The diagnosis is based on the comparison with
                                              standardized empirical, qualitative assessments of limited objectivity.
                                              We propose an approach to pathology based on interferometric imaging of
                                              “unstained” biopsies, which provides unique capabilities for quantitative
                                              diagnosis and automation. We developed a label-free tissue scanner based
                                              on “quantitative phase imaging,” which maps out optical path length at
                                              each point in the field of view and, thus, yields images that are sensitive
                                              to the “nanoscale” tissue architecture. Unlike analysis of stained tissue,
                                              which is qualitative in nature and affected by color balance, staining
                                              strength and imaging conditions, optical path length measurements are
                                              intrinsically quantitative, i.e., images can be compared across different
                                              instruments and clinical sites. These critical features allow us to automate
                                              the diagnosis process. We paired our interferometric optical system with highly
                                              parallelized, dedicated software algorithms for data acquisition, allowing us
                                              to image at a throughput comparable to that of commercial tissue scanners while
                                              maintaining the nanoscale sensitivity to morphology. Based on the measured phase
                                              information, we implemented software tools for autofocusing during imaging, as
                                              well as image archiving and data access. To illustrate the potential of our
                                              technology for large volume pathology screening, we established an “intrinsic
                                              marker” for colorectal disease that detects tissue with dysplasia or colorectal
                                              cancer and flags specific areas for further examination, potentially improving
                                              the efficiency of existing pathology workflows. ">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://www.spiedigitallibrary.org/journals/Journal_of_Biomedical_Optics/volume-22/issue-6/066016/Label-free-tissue-scanner-for-colorectal-cancer-screening/10.1117/1.JBO.22.6.066016.full">PDF & Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/majeed2016towards.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Towards Quantitative Automated Histopathology of Breast Cancer using Spatial Light Interference Microscopy
            (SLIM)</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Hassaan Majeed, Tan H Nguyen, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias,
            Krishnarao Tangella, Andre Balla, Minh N Do, Gabriel Popescu</p>
          <p class="text-left">United States and Canadian Academy of Pathology (USCAP), Seattle, WA, March 12-18, 2016</p>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/majeed2015breast.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Breast Cancer Diagnosis using Spatial Light Interference Microscopy</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Hassaan Majeed, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias, Krishnarao Tangella,
            Andre Balla, Gabriel Popescu</p>
          <p class="text-left">Journal of Biomedical Optics, Opt. 20(11), 2015</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content=
               "The standard practice in histopathology of breast cancers is to examine a hematoxylin and eosin (H&E) stained tissue biopsy under a microscope to diagnose whether a lesion is benign or malignant. This determination is made based on a manual, qualitative inspection, making it subject to investigator bias and resulting in low throughput. Hence, a quantitative, label-free, and high-throughput diagnosis method is highly desirable. We present here preliminary results showing the potential of quantitative phase imaging for breast cancer screening and help with differential diagnosis. We generated phase maps of unstained breast tissue biopsies using spatial light interference microscopy (SLIM). As a first step toward quantitative diagnosis based on SLIM, we carried out a qualitative evaluation of our label-free images. These images were shown to two pathologists who classified each case as either benign or malignant. This diagnosis was then compared against the diagnosis of the two pathologists on corresponding H&E stained tissue images and the number of agreements were counted. The agreement between SLIM and H&E based diagnosis was 88% for the first pathologist and 87% for the second. Our results demonstrate the potential and promise of SLIM for quantitative, label-free, and high-throughput diagnosis."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank"
               href="http://light.ece.illinois.edu/wp-content/uploads/2015/10/Hassaan_JBO_20_11_111210.pdf">PDF</a>
            <a class="btn btn-default btn-xs" target="_blank"
               href="http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=2430724">Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/majeed2015high.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">High Throughput Imaging of Blood Smears using White Light Diffraction Phase Microscopy</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Hassaan Majeed, Mikhail E Kandel, Basanta Bhaduri, Kevin Han, <strong>Zelun Luo</strong>, Krishnarao Tangella,
            Gabriel Popescu</p>
          <p class="text-left">SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content=
               "While automated blood cell counters have made great progress in detecting abnormalities in blood, the lack of specificity for a particular disease, limited information on single cell morphology and intrinsic uncertainly due to high throughput in these instruments often necessitates detailed inspection in the form of a peripheral blood smear. Such tests are relatively time consuming and frequently rely on medical professionals tally counting specific cell types. These assays rely on the contrast generated by chemical stains, with the signal intensity strongly related to staining and preparation techniques, frustrating machine learning algorithms that require consistent quantities to denote the features in question. Instead we opt to use quantitative phase imaging, understanding that the resulting image is entirely due to the structure (intrinsic contrast) rather than the complex interplay of stain and sample. We present here our first steps to automate peripheral blood smear scanning, in particular a method to generate the quantitative phase image of an entire blood smear at high throughput using white light diffraction phase microscopy (wDPM), a single shot and common path interferometric imaging technique."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204110">
              PDF & Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/majeed2015diagnosis.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">Diagnosis of Breast Cancer Biopsies using Quantitative Phase Imaging</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Hassaan Majeed, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias, Krishnarao Tangella,
            Andre Balla, Gabriel Popescu</p>
          <p class="text-left">SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               title="Abstract" data-content=
               "The standard practice in the histopathology of breast cancers is to examine a hematoxylin and eosin (H&E) stained tissue biopsy under a microscope. The pathologist looks at certain morphological features, visible under the stain, to diagnose whether a tumor is benign or malignant. This determination is made based on qualitative inspection making it subject to investigator bias. Furthermore, since this method requires a microscopic examination by the pathologist it suffers from low throughput. A quantitative, label-free and high throughput method for detection of these morphological features from images of tissue biopsies is, hence, highly desirable as it would assist the pathologist in making a quicker and more accurate diagnosis of cancers. We present here preliminary results showing the potential of using quantitative phase imaging for breast cancer screening and help with differential diagnosis. We generated optical path length maps of unstained breast tissue biopsies using Spatial Light Interference Microscopy (SLIM). As a first step towards diagnosis based on quantitative phase imaging, we carried out a qualitative evaluation of the imaging resolution and contrast of our label-free phase images. These images were shown to two pathologists who marked the tumors present in tissue as either benign or malignant. This diagnosis was then compared against the diagnosis of the two pathologists on H&E stained tissue images and the number of agreements were counted. In our experiment, the agreement between SLIM and H&E based diagnosis was measured to be 88%. Our preliminary results demonstrate the potential and promise of SLIM for a push in the future towards quantitative, label-free and high throughput diagnosis."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204106">
              PDF & Website</a>
          </div>
        </div>
      </div>

      <div class="row paper">
        <div class="col-xs-3">
          <img src="publications/kandel2015cpp.png" class="paper-fig img-responsive">
        </div>
        <div class="col-xs-9">
          <h4 class="text-left">C++ Software Integration for a High-throughput Phase Imaging Platform</h4>
          <p class="text-left">
            <span class="label label-success">Bioimaging</span>
            <span class="label label-success">Medical Image Analysis</span>
          </p>
          <p class="text-left">Mikhail E Kandel, <strong>Zelun Luo</strong>, Kevin Han, Gabriel Popescu</p>
          <p class="text-left">SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</p>
          <div class="text-left btns">
            <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
               data-html="True" title="Abstract" data-content=
               "The multi-shot approach in SLIM requires reliable, synchronous, and parallel operation of three independent hardware devices – not meeting these challenges results in degraded phase and slow acquisition speeds, narrowing applications to holistic statements about complex phenomena. The relative youth of quantitative imaging and the lack of ready-made commercial hardware and tools further compounds the problem as Higher level programming languages result in inflexible, experiment specific instruments limited by ill-fitting computational modules, resulting in a palpable chasm between promised and realized hardware performance. Furthermore, general unfamiliarity with intricacies such as background calibration, objective lens attenuation, along with spatial light modular alignment, makes successful measurements difficult for the inattentive or uninitiated. This poses an immediate challenge for moving our techniques beyond the lab to biologically oriented collaborators and clinical practitioners.
                <br />To meet these challenges, we present our new Quantitative Phase Imaging pipeline, with improved instrument performance, friendly user interface and robust data processing features, enabling us to acquire and catalog clinical datasets hundreds of gigapixels in size."
            >Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204094">
              PDF & Website</a>
          </div>
        </div>
      </div>
    </div>


  </div>
</section>

<!-- Contact Section -->
<section id="contact" class="contact-section">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <h1 class="subtitle">Projects</h1>
      </div>
    </div>

    <div class="row project-pair">
      <div class="col-xs-5">
        <div class="project">
          <img src="projects/text.png" class="project-fig img-responsive">
          <div class="project-text">
            <h4>End-to-End Deep Neural Network <br/> for Scene Text Recognition</h4>
            <p>Internship Project <br/> Visual Search, Amazon A9 <br/> Advisors: <a href="https://www.linkedin.com/in/sontran">Dr. Son Tran</a>, <a href="http://ciir.cs.umass.edu/~manmatha/">Dr. R. Manmatha</a></p>
          </div>
        </div>
      </div>
      <div class="col-xs-5 col-xs-offset-2">
        <div class="project">
          <img src="projects/superstitchous.png" class="project-fig img-responsive">
          <div class="project-text">
            <h4>Superstitchous: <br/> Image Stitching and Core Segmentation Software for Large Scale Digital Holography</h4>
            <p>Senior Design <br/> Advisor: <a href="http://light.ece.illinois.edu/">Prof. Gabriel Popescu</a></p>
            <div>
              <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right"
                 data-html="True" title="Abstract" data-content=
                 "We implemented an image stitching program for quantitative phase microscope images, as well as a  web-based image viewer and core segmentation program. The image stitching program is able to stitch  entire slides consisting of hundreds of gigabytes of data, and the user can upload them to our server and  view them remotely. Zooming and panning of images is supported. Finally, biopsy cores in the image can  be automatically identified and output into separate images."
              >Abstract</a>
              <a class="btn btn-default btn-xs" target="_blank" href="projects/senior_design.pdf">PDF</a>
              <a class="btn btn-default btn-xs" target="_blank" href="https://github.com/d1ngn1gefe1/superstitchous2.0">Code</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="row project-pair">
      <div class="col-xs-5">
        <div class="project">
          <img src="projects/navi.png" class="project-fig img-responsive">
          <div class="project-text">
            <h4>Navi: </br> Your In-Store Navigator</h4>
            <div>
              <a class="btn btn-default btn-xs" target="_blank" href="projects/navi1.pdf">Report</a>
              <a class="btn btn-default btn-xs" target="_blank" href="projects/navi2.pdf">Interface</a>
              <a class="btn btn-default btn-xs" target="_blank" href="https://www.youtube.com/watch?v=FNEQI18o1J8">Demo</a>
            </div>
          </div>
        </div>
      </div>
      <div class="col-xs-5 col-xs-offset-2">
        <div class="project">
          <img src="projects/deep-annotator.png" class="project-fig img-responsive">
          <div class="project-text">
            <h4>Deep Annotator: <br/> A General Purpose Web Annotation Interface</h4>
            <div>
              <a class="btn btn-default btn-xs" target="_blank" href="https://youtu.be/SQ1KmHaRJiQ">Demo</a>
              <a class="btn btn-default btn-xs" target="_blank" href="https://github.com/d1ngn1gefe1/deepannotator">Code</a>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="row project-pair">
      <div class="col-xs-5">
        <div class="project">
          <img src="projects/captioning.png" class="project-fig img-responsive">
          <div class="project-text">
            <h4>Show, Discriminate, and Tell: <br/> A Discriminative Image Captioning Model <br/> with Deep Neural Networks</h4>
            <div>
              <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="right" data-html="True" title="Abstract" data-content="Caption generation has long been seen as a difficult problem in Computer Vision and Natural Language Processing. In this paper, we present an image captioning model based on a end-to-end neural framework that combines Convolutional Neural Network and Recurrent Neural Network. Critical to our approach is a ranking objective that attempts to add discriminatory power to the model. Experiments on MS COCO dataset shows that our model consistently outperforms its counterpart with no ranking objective, both quantitatively based on BLEU and CIDEr scores and qualitatively based on human evaluation.">Abstract</a>
            <a class="btn btn-default btn-xs" target="_blank" href="https://web.stanford.edu/class/cs231a/prev_projects_2016/cs231a.pdf">PDF</a>
            </div>
          </div>
        </div>
      </div>
      <div class="col-xs-5 col-xs-offset-2">
<!--         <div class="project">
        </div> -->
      </div>
    </div>

  </div>
</section>

<!--Google Analytics-->
<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
    a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-100690611-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- jQuery -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"
        integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Custom JavaScript -->
<script src="js/effects.js"></script>

</body>

</html>
