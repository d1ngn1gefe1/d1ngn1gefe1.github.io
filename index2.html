<div class="row">
  <h3 class="subfield-first">Trustworthy AI</h3>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/luo2021scalable.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Scalable Differential Privacy with Sparse Network Fine-tuning</h4>
      <p><strong>Zelun Luo</strong>, Daniel Wu, Ehsan Adeli, Li Fei-Fei</p>
      <p>Conference on Computer Vision and Pattern Recognition (CVPR) 2021</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content=
               "We propose a novel method for privacy-preserving training of deep neural networks leveraging public, out-domain data. While differential privacy (DP) has emerged as a mechanism to protect sensitive data in training datasets, its application to complex visual recognition tasks remains challenging. Traditional DP methods, such as Differentially-Private Stochastic Gradient Descent (DP-SGD), only perform well on simple datasets and shallow networks, while recent transfer learning-based DP methods often make unrealistic assumptions about the availability and distribution of public data. In this work, we argue that minimizing the number of trainable parameters is the key to improving the privacy-performance tradeoff of DP on complex visual recognition tasks. We also propose a novel transfer learning paradigm that finetunes a very sparse subnetwork with DP, inspired by this argument. We conduct extensive experiments and ablation studies on two visual recognition tasks: CIFAR-100 -> CIFAR-10 (standard DP setting) and the CD-FSL challenge (few-shot, multiple levels of domain shifts) and demonstrate competitive experimental performance."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="">PDF</a>
      </div>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/martinezmartin2021ethical.jpg" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Ethical Issues in Using Ambient Intelligence in Health-care Settings</h4>
      <p>Nicole Martinez-Martin, <strong>Zelun Luo</strong>, Amit Kaushal, Ehsan Adeli, Albert Haque, Sara S Kelly, Sarah Wieten, Mildred K Cho, David Magnus, Li Fei-Fei, Kevin Schulman, Arnold Milstein</p>
      <p>The Lancet Digital Health, Volume 3, Issue 2, February 2021</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content="Ambient intelligence is increasingly finding applications in health-care settings, such as helping to ensure clinician and patient safety by monitoring staff compliance with clinical best practices or relieving staff of burdensome documentation tasks. Ambient intelligence involves using contactless sensors and contact-based wearable devices embedded in health-care settings to collect data (eg, imaging data of physical spaces, audio data, or body temperature), coupled with machine learning algorithms to efficiently and effectively interpret these data. Despite the promise of ambient intelligence to improve quality of care, the continuous collection of large amounts of sensor data in health-care settings presents ethical challenges, particularly in terms of privacy, data management, bias and fairness, and informed consent. Navigating these ethical issues is crucial not only for the success of individual uses, but for acceptance of the field as a whole."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/publications/martinezmartin2021ethical.pdf">PDF</a>
      </div>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/luo2017label.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Label Efficient Learning of Transferable Representations across Domains and Tasks</h4>
      <p><strong>Zelun Luo</strong>, Yuliang Zou, Judy Hoffman, Li Fei-Fei</p>
      <p>Conference on Neural Information Processing Systems (NIPS) 2017</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content=
               "We propose a framework that learns a representation transferable across different domains and tasks in a data efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/1712.00123">PDF</a>
        <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/nips17_label/poster.pdf">Poster</a>
        <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/nips17_label/">Project</a>
      </div>
    </div>
  </div>
</div>

<div class="row">
  <h3 class="subfield-first">Computer Vision and Deep Learning</h3>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/luo2018graph.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Graph Distillation for Action Detection with Privileged Information</h4>
      <p><strong>Zelun Luo</strong>, Jun-Ting Hsieh, Lu Jiang, Juan Carlos Niebles, Li Fei-Fei</p>
      <p>European Conference on Computer Vision (ECCV) 2018</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content=>Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/1712.00108">PDF</a>
        <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/eccv18_graph">Project</a>
        <a class="btn btn-default btn-xs" target="_blank" href="http://alan.vision/publications/luo2018graph_poster.pdf">Poster</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://github.com/google/graph_distillation">Code</a>
      </div>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/zou2018dfnet.gif" class="img-fluid" onerror="this.onerror=null;this.src='publications/zou2018dfnet.gif';">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Network Consistency</h4>
      <p>Yuliang Zou, <strong>Zelun Luo</strong>, Jia-Bin Huang</p>
      <p>European Conference on Computer Vision (ECCV) 2018</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content="We present an unsupervised learning framework for simultaneously training single-view depth prediction and optical flow estimation models using unlabeled video sequences. Existing unsupervised methods often exploit brightness constancy and spatial smoothness priors to train depth or flow models. In this paper, we propose to leverage geometric consistency as additional supervisory signals. Our core idea is that for rigid regions we can use the predicted scene depth and camera motion to synthesize 2D optical flow by backprojecting the induced 3D scene flow. The discrepancy between the rigid flow (from depth prediction and camera motion) and the estimated flow (from optical flow model) allows us to impose a cross-task consistency loss. While all the networks are jointly optimized during training, they can be applied independently at test time. Extensive experiments demonstrate that our depth and flow models compare favorably with state-of-the-art unsupervised methods."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/abs/1809.01649">PDF</a>
        <a class="btn btn-default btn-xs" target="_blank" href="http://yuliang.vision/DF-Net/">Project</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://github.com/vt-vl-lab/DF-Net">Code</a>
      </div>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/luo2017unsupervised.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Unsupervised Learning of Long-Term Motion Dynamics for Videos</h4>
      <p><strong>Zelun Luo</strong>, Boya Peng, De-An Huang, Alexandre Alahi, Li Fei-Fei</p>
      <p>Conference on Computer Vision and Pattern Recognition (CVPR) 2017</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content="We present an unsupervised representation learning approach
                                              that compactly encodes the motion dependencies
                                              in videos. Given a pair of images from a video clip, our
                                              framework learns to predict the long-term 3D motions. To
                                              reduce the complexity of the learning framework, we propose
                                              to describe the motion as a sequence of atomic 3D
                                              flows computed with RGB-D modality. We use a Recurrent
                                              Neural Network based Encoder-Decoder framework
                                              to predict these sequences of flows. We argue that in order
                                              for the decoder to reconstruct these sequences, the encoder
                                              must learn a robust video representation that captures
                                              long-term motion dependencies and spatial-temporal relations.
                                              We demonstrate the effectiveness of our learned temporal
                                              representations on activity classification across multiple
                                              modalities and datasets such as NTU RGB+D and MSR
                                              Daily Activity 3D. Our framework is generic to any input
                                              modality, i.e., RGB, depth, and RGB-D videos.">Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1701.01821.pdf">PDF</a>
        <a class="btn btn-default btn-xs" target="_blank" href="publications/CVPR2017-poster.png">Poster</a>
      </div>
    </div>
  </div>

  <div class="row paper paper-last">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/haque2016towards.gif" class="img-fluid" onerror="this.onerror=null;this.src='publications/haque2016towards.png';">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Towards Viewpoint Invariant 3D Human Pose Estimation</h4>
      <p>Albert Haque, <strong>Zelun Luo*</strong>, Boya Peng*, Alexandre Alahi, Serena Yeung, Li Fei-Fei</p>
      <p>European Conference on Computer Vision (ECCV) 2016</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content="We propose a viewpoint invariant model for 3D human pose
                                              estimation from a single depth image. To achieve this, our discriminative
                                              model embeds local regions into a learned viewpoint invariant feature
                                              space. Formulated as a multi-task learning problem, our model is able to
                                              selectively predict partial poses in the presence of noise and occlusion.
                                              Our approach leverages a convolutional and recurrent network architecture
                                              with a top-down error feedback mechanism to self-correct previous
                                              pose estimates in an end-to-end manner. We evaluate our model on a
                                              previously published depth dataset and a newly collected human pose
                                              dataset containing 100K annotated depth images from extreme viewpoints.
                                              Experiments show that our model achieves competitive performance
                                              on frontal views while achieving state-of-the-art performance on
                                              alternate viewpoints.">Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1603.07076.pdf">PDF</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://www.alberthaque.com/projects/viewpoint_3d_pose/">Website</a>
      </div>
    </div>
  </div>
</div>

<div class="row">
  <h3 class="subfield">AI-Assisted Healthcare</h3>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/darke2018vision.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Vision-Based Gait Analysis for Senior Care</h4>
      <p>Evan Darke, Anin Sayana, Kelly Shen, David Xue, Jun-Ting Hsieh, <strong>Zelun Luo</strong>, Li-Jia Li, N, Lance Downing, Arnold Milstein, Li Fei-Fei</p>
      <p>ML4H: Machine Learning for Health, NeurIPS 2018, Montreal, Canada, December 8, 2018</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content="As the senior population rapidly increases, it is challenging yet crucial to provide effective long-term care for seniors who live at home or in senior care facilities. Smart senior homes, which have gained widespread interest in the healthcare community, have been proposed to improve the well-being of seniors living independently. In particular, non-intrusive, cost-effective sensors placed in these senior homes enable gait characterization, which can provide clinically relevant information including mobility level and early neurodegenerative disease risk. In this paper, we present a method to perform gait analysis from a single camera placed within the home. We show that we can accurately calculate various gait parameters, demonstrating the potential for our system to monitor the long-term gait of seniors and thus aid clinicians in understanding a patient’s medical profile."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1812.00169.pdf">PDF</a>
      </div>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/luo2018computer.gif" class="img-fluid" onerror="this.onerror=null;this.src='publications/luo2018computer.gif';">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Computer Vision-based Descriptive Analytics of Seniors' Daily Activities for Long-term Health Monitoring</h4>
      <p><strong>Zelun Luo*</strong>, Jun-Ting Hsieh*, Niranjan Balachandar, Serena Yeung, Guido Pusiol, Jay Luxenberg, Grace Li, Li-Jia Li, N. Lance Downing, Arnold Milstein, Li Fei-Fei</p>
      <p>Machine Learning for Healthcare (MLHC) 2018, Stanford, CA, August 17-18, 2018</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content="One in twenty-five patients admitted to a hospital will suffer from a hospital acquired infection. If we can intelligently track healthcare staff, patients, and visitors, we can better understand the sources of such infections. We envision a smart hospital capable of increasing operational efficiency and improving patient care with less spending. In this paper, we propose a non-intrusive vision-based system for tracking people’s activity in hospitals. We evaluate our method for the problem of measuring hand hygiene compliance. Empirically, our method outperforms existing solutions such as proximity-based techniques and covert in-person observational studies. We present intuitive, qualitative results that analyze human movement patterns and conduct spatial analytics which convey our method’s interpretability. This work is a first step towards a computer-vision based smart hospital and demonstrates promising results for reducing hospital acquired infections."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/5b7373254ae23704e284bdf4/1534292778467/18.pdf">PDF</a>
      </div>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/haque2017towards.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene Compliance</h4>
      <p>Albert Haque, Michelle Guo, Alexandre Alahi, Serena Yeung, <strong>Zelun Luo</strong>, Alisha Rege, Amit Singh, Jeffrey Jopling, N. Lance Downing, William Beninati, Terry Platchek, Arnold Milstein, Li Fei-Fei</p>
      <p>Machine Learning for Healthcare (MLHC) 2017, Boston, MA, August 18-19, 2017</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content="Nations around the world face rising demand for costly long-term care for seniors. Patterns
                                              in seniors' activities of daily living, such as sleeping, sitting, standing, walking, etc. can
                                              provide caregivers useful clues regarding seniors' health. As the senior population continues
                                              to grow worldwide, continuous manual monitoring of seniors' daily activities will
                                              become more and more challenging for caregivers. Thus to improve caregivers' ability
                                              to assist seniors, an automated system for monitoring and analyzing patterns in seniors
                                              activities of daily living would be useful. A possible approach to implementing such a
                                              system involves wearable sensors, but this approach is intrusive and requires adherence by
                                              patients. In this paper, using a dataset we collected from an assisted-living facility for
                                              seniors, we present a novel computer vision-based approach that leverages nonintrusive,
                                              privacy-compliant multi-modal sensors and state-of-the-art computer vision techniques for
                                              continuous activity detection to remotely detect and provide long-term descriptive analytics
                                              of senior activities. These analytics include both qualitative and quantitative descriptions
                                              of senior daily activity patterns that can be interpreted by caregivers. Our work is progress
                                              towards a smart senior home that uses computer vision to support caregivers in senior
                                              healthcare to help meet the challenges of an aging worldwide population.">Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://arxiv.org/pdf/1708.00163.pdf">PDF</a>
      </div>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/luo2017computer.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Computer Vision-based Approach to Maintain Independent Living for Seniors</h4>
      <p><strong>Zelun Luo</strong>, Alisha Rege, Guido Pusiol, Arnold Milstein, Li Fei-Fei, N. Lance Downing</p>
      <p>American Medical Informatics Association (AMIA), Washington, DC, November 4-8, 2017</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content=
               "Recent progress in developing cost-effective sensors and machine learning techniques has enabled new AI-assisted solutions for human behavior understanding. In this work, we investigate the use of thermal and depth sensors for the detection of daily activities, lifestyle patterns, emotions, and vital signs, as well as the development of intelligent mechanisms for accurate situational assessment and rapid response. We demonstrate an integrated solution for remote monitoring, assessment, and support of seniors living independently at home."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="publications/AMIA-Poster.pdf">Poster</a>
        <a class="btn btn-default btn-xs" target="_blank" href="publications/luo2017computer.pdf">Manuscript</a>
      </div>
    </div>
  </div>

  <div class="row paper paper-last">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/yeung2015vision.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Vision-Based Hand Hygiene Monitoring in Hospitals</h4>
      <p>Serena Yeung, Alexandre Alahi, <strong>Zelun Luo</strong>, Boya Peng, Albert Haque, Amit Singh, Terry Platchek,
        Arnold Milstein, Li Fei-Fei</p>
      <p>American Medical Informatics Association (AMIA), Chicago, November 12-16, 2016</p>
      <p>NIPS Workshop on Machine Learning for Healthcare, 2015</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content="Recent progress in developing cost-effective depth sensors has enabled new AI-assisted
                                              solutions such as assisted driving vehicles and smart spaces. Machine
                                              learning techniques have been successfully applied on these depth signals to perceive
                                              meaningful information about human behavior. In this work, we propose
                                              to deploy depth sensors in hospital settings and use computer vision methods to
                                              enable AI-assisted care. We aim to reduce visually-identifiable human errors such
                                              as hand hygiene compliance, one of the leading causes of Health Care-Associated
                                              Infection (HCAI) in hospitals."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="http://ai.stanford.edu/~syyeung/resources/vision_hand_hh_nipsmlhc.pdf">PDF</a>
      </div>
    </div>
  </div>
</div>

<div class="row">
  <h3 class="subfield">Biomedical Imaging and Diagnosis</h3>
  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/kandel2017label.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Label-Free Tissue Scanner for Colorectal Cancer Screening</h4>
      <p>Mikhail E. Kandel, Shamira Sridharan, Jon Liang, <strong>Zelun Luo</strong>, Kevin Han, Virgilia Macias, Anish Shah,
        Roshan Patel, Krishnarao Tangella, Andre Kajdacsy-Balla, Grace Guzman, Gabriel Popescu</p>
      <p>Journal of Biomedical Optics, Opt. 22(6), 2017</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content="The current practice of surgical pathology relies on external contrast
                                              agents to reveal tissue architecture, which is then qualitatively examined
                                              by a trained pathologist. The diagnosis is based on the comparison with
                                              standardized empirical, qualitative assessments of limited objectivity.
                                              We propose an approach to pathology based on interferometric imaging of
                                              “unstained” biopsies, which provides unique capabilities for quantitative
                                              diagnosis and automation. We developed a label-free tissue scanner based
                                              on “quantitative phase imaging,” which maps out optical path length at
                                              each point in the field of view and, thus, yields images that are sensitive
                                              to the “nanoscale” tissue architecture. Unlike analysis of stained tissue,
                                              which is qualitative in nature and affected by color balance, staining
                                              strength and imaging conditions, optical path length measurements are
                                              intrinsically quantitative, i.e., images can be compared across different
                                              instruments and clinical sites. These critical features allow us to automate
                                              the diagnosis process. We paired our interferometric optical system with highly
                                              parallelized, dedicated software algorithms for data acquisition, allowing us
                                              to image at a throughput comparable to that of commercial tissue scanners while
                                              maintaining the nanoscale sensitivity to morphology. Based on the measured phase
                                              information, we implemented software tools for autofocusing during imaging, as
                                              well as image archiving and data access. To illustrate the potential of our
                                              technology for large volume pathology screening, we established an “intrinsic
                                              marker” for colorectal disease that detects tissue with dysplasia or colorectal
                                              cancer and flags specific areas for further examination, potentially improving
                                              the efficiency of existing pathology workflows. ">Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="https://www.spiedigitallibrary.org/journals/Journal_of_Biomedical_Optics/volume-22/issue-6/066016/Label-free-tissue-scanner-for-colorectal-cancer-screening/10.1117/1.JBO.22.6.066016.full">PDF & Website</a>
      </div>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/majeed2016towards.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Towards Quantitative Automated Histopathology of Breast Cancer using Spatial Light Interference Microscopy (SLIM)</h4>
      <p>Hassaan Majeed, Tan H Nguyen, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias,
        Krishnarao Tangella, Andre Balla, Minh N Do, Gabriel Popescu</p>
      <p>United States and Canadian Academy of Pathology (USCAP), Seattle, WA, March 12-18, 2016</p>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/majeed2015breast.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Breast Cancer Diagnosis using Spatial Light Interference Microscopy</h4>
      <p>Hassaan Majeed, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias, Krishnarao Tangella,
        Andre Balla, Gabriel Popescu</p>
      <p>Journal of Biomedical Optics, Opt. 20(11), 2015</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content=
               "The standard practice in histopathology of breast cancers is to examine a hematoxylin and eosin (H&E) stained tissue biopsy under a microscope to diagnose whether a lesion is benign or malignant. This determination is made based on a manual, qualitative inspection, making it subject to investigator bias and resulting in low throughput. Hence, a quantitative, label-free, and high-throughput diagnosis method is highly desirable. We present here preliminary results showing the potential of quantitative phase imaging for breast cancer screening and help with differential diagnosis. We generated phase maps of unstained breast tissue biopsies using spatial light interference microscopy (SLIM). As a first step toward quantitative diagnosis based on SLIM, we carried out a qualitative evaluation of our label-free images. These images were shown to two pathologists who classified each case as either benign or malignant. This diagnosis was then compared against the diagnosis of the two pathologists on corresponding H&E stained tissue images and the number of agreements were counted. The agreement between SLIM and H&E based diagnosis was 88% for the first pathologist and 87% for the second. Our results demonstrate the potential and promise of SLIM for quantitative, label-free, and high-throughput diagnosis."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank"
           href="http://light.ece.illinois.edu/wp-content/uploads/2015/10/Hassaan_JBO_20_11_111210.pdf">PDF</a>
        <a class="btn btn-default btn-xs" target="_blank"
           href="http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=2430724">Website</a>
      </div>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/majeed2015high.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>High Throughput Imaging of Blood Smears using White Light Diffraction Phase Microscopy</h4>
      <p>Hassaan Majeed, Mikhail E Kandel, Basanta Bhaduri, Kevin Han, <strong>Zelun Luo</strong>, Krishnarao Tangella,
        Gabriel Popescu</p>
      <p>SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content=
               "While automated blood cell counters have made great progress in detecting abnormalities in blood, the lack of specificity for a particular disease, limited information on single cell morphology and intrinsic uncertainly due to high throughput in these instruments often necessitates detailed inspection in the form of a peripheral blood smear. Such tests are relatively time consuming and frequently rely on medical professionals tally counting specific cell types. These assays rely on the contrast generated by chemical stains, with the signal intensity strongly related to staining and preparation techniques, frustrating machine learning algorithms that require consistent quantities to denote the features in question. Instead we opt to use quantitative phase imaging, understanding that the resulting image is entirely due to the structure (intrinsic contrast) rather than the complex interplay of stain and sample. We present here our first steps to automate peripheral blood smear scanning, in particular a method to generate the quantitative phase image of an entire blood smear at high throughput using white light diffraction phase microscopy (wDPM), a single shot and common path interferometric imaging technique."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204110">
          PDF & Website</a>
      </div>
    </div>
  </div>

  <div class="row paper">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/majeed2015diagnosis.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>Diagnosis of Breast Cancer Biopsies using Quantitative Phase Imaging</h4>
      <p>Hassaan Majeed, Mikhail E Kandel, Kevin Han, <strong>Zelun Luo</strong>, Virgilia Macias, Krishnarao Tangella,
        Andre Balla, Gabriel Popescu</p>
      <p>SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           title="Abstract" data-content=
               "The standard practice in the histopathology of breast cancers is to examine a hematoxylin and eosin (H&E) stained tissue biopsy under a microscope. The pathologist looks at certain morphological features, visible under the stain, to diagnose whether a tumor is benign or malignant. This determination is made based on qualitative inspection making it subject to investigator bias. Furthermore, since this method requires a microscopic examination by the pathologist it suffers from low throughput. A quantitative, label-free and high throughput method for detection of these morphological features from images of tissue biopsies is, hence, highly desirable as it would assist the pathologist in making a quicker and more accurate diagnosis of cancers. We present here preliminary results showing the potential of using quantitative phase imaging for breast cancer screening and help with differential diagnosis. We generated optical path length maps of unstained breast tissue biopsies using Spatial Light Interference Microscopy (SLIM). As a first step towards diagnosis based on quantitative phase imaging, we carried out a qualitative evaluation of the imaging resolution and contrast of our label-free phase images. These images were shown to two pathologists who marked the tumors present in tissue as either benign or malignant. This diagnosis was then compared against the diagnosis of the two pathologists on H&E stained tissue images and the number of agreements were counted. In our experiment, the agreement between SLIM and H&E based diagnosis was measured to be 88%. Our preliminary results demonstrate the potential and promise of SLIM for a push in the future towards quantitative, label-free and high throughput diagnosis."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204106">
          PDF & Website</a>
      </div>
    </div>
  </div>

  <div class="row paper paper-last">
    <div class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig">
      <img src="publications/kandel2015cpp.png" class="img-fluid">
    </div>
    <div class="col-sm-9 col-xs-12 paper-info">
      <h4>C++ Software Integration for a High-throughput Phase Imaging Platform</h4>
      <p>Mikhail E Kandel, <strong>Zelun Luo</strong>, Kevin Han, Gabriel Popescu</p>
      <p>SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</p>
      <div>
        <a tabindex="0" class="btn btn-default btn-xs" role="button" data-toggle="popover" data-trigger="hover" data-placement="bottom"
           data-html="True" title="Abstract" data-content=
               "The multi-shot approach in SLIM requires reliable, synchronous, and parallel operation of three independent hardware devices – not meeting these challenges results in degraded phase and slow acquisition speeds, narrowing applications to holistic statements about complex phenomena. The relative youth of quantitative imaging and the lack of ready-made commercial hardware and tools further compounds the problem as Higher level programming languages result in inflexible, experiment specific instruments limited by ill-fitting computational modules, resulting in a palpable chasm between promised and realized hardware performance. Furthermore, general unfamiliarity with intricacies such as background calibration, objective lens attenuation, along with spatial light modular alignment, makes successful measurements difficult for the inattentive or uninitiated. This poses an immediate challenge for moving our techniques beyond the lab to biologically oriented collaborators and clinical practitioners.
                <br />To meet these challenges, we present our new Quantitative Phase Imaging pipeline, with improved instrument performance, friendly user interface and robust data processing features, enabling us to acquire and catalog clinical datasets hundreds of gigapixels in size."
        >Abstract</a>
        <a class="btn btn-default btn-xs" target="_blank" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204094">
          PDF & Website</a>
      </div>
    </div>
  </div>
</div>